---
title: "Investment"
author: "Pallak Goyal"
date: "2024-01-09"
output: word_document
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r code, include=FALSE}
# will run a panel data regression 
# dependent variable is investment_it/capital_it-1
# explanatory variables
# cashflow_it/capital_it-1
# sales_it-1/capital_it-1
# debt_it/capital_it-1
# uncertainty_t-1
# pli_t
# first will load data from ca and sa 
# loading data from ca
ca <- read.table("./investment_cat1_ca.txt", header = T, sep = "|", na.strings = "", comment.char = "", quote = "\"", fill = F, nrows = 238)
# setting the right col type
ca[,1] <- as.character(ca[,1])
ca[,3] <- format(as.Date(as.character(ca[,3]), "%Y%m%d"), "%Y")
for (i in 6:12){
  ca[,i] <- as.numeric(ca[,i])
}
# imputing missing data
# imputing zeros for missing values of current portion of long term borrowings
for (i in 1:nrow(ca)){
  if(is.na(ca[i,8]) == TRUE){
    ca[i,8] <- 0
  }
}
# imputing zero for missing value of gross addition to gross fixed assets
for (i in 1:nrow(ca)){
  if(is.na(ca[i,10]) == TRUE){
    ca[i,10] <- 0
  }
}
# imputing zero for missing values of total additional gross intangible assets
for (i in 1:nrow(ca)){
  if(is.na(ca[i,11]) == TRUE){
    ca[i,11] <- 0
  }
}
# imputing other missing values with entity level averages
for (i in 6:ncol(ca)){
  ca[,i] <- ave(ca[[i]], ca$ca_finance1_cocode, FUN = function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
}
# inserting column for investment
for (i in 1:nrow(ca)){
  ca$investment[i] <- ca[i,10]-ca[i,11]
}
# inserting column for debt
for (i in 1:nrow(ca)){
  ca$debt[i] <- ca[i,7] + ca[i,8]
}
# getting rid of useless columns
ca <- ca[,-c(4,5,7,8,10,11)]
# loading data from sa
sa <- read.table("./investment_cat1_sa.txt", header = T, sep = "|", na.strings = "", comment.char = "", quote = "\"", fill = F, nrows = 387)
# setting the right col type
sa[,1] <- as.character(sa[,1])
sa[,3] <- format(as.Date(as.character(sa[,3]), "%Y%m%d"), "%Y")
for (i in 6:12){
  sa[,i] <- as.numeric(sa[,i])
}
# imputing missing data
# imputing zeros for missing values of current portion of long term borrowings
for (i in 1:nrow(sa)){
  if(is.na(sa[i,8]) == TRUE){
    sa[i,8] <- 0
  }
}
# imputing zero for missing value of gross addition to gross fixed assets
for (i in 1:nrow(sa)){
  if(is.na(sa[i,9]) == TRUE){
    sa[i,10] <- 0
  }
}
# imputing zero for missing values of total additional gross intangible assets
for (i in 1:nrow(sa)){
  if(is.na(sa[i,10]) == TRUE){
    sa[i,10] <- 0
  }
}
# imputing other missing values with entity level averages
for (i in 6:ncol(sa)){
  sa[,i] <- ave(sa[[i]], sa$sa_finance1_cocode, FUN = function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
}
# inserting column for investment
for (i in 1:nrow(sa)){
  sa$investment[i] <- sa[i,9]-sa[i,10]
}
# inserting column for debt
for (i in 1:nrow(sa)){
  sa$debt[i] <- sa[i,7] + sa[i,8]
}
# getting ride of useless column in sa
sa <- sa[,-c(4,5,7,8,9,10)]
# will merge sa and ca data
# will keep ca data wherever availale and fill the rest with sa data
# creating new id for ca 
for (i in 1:nrow(ca)){
  ca$id[i] <- paste(ca[i,1], ca[i,3], sep = "_")
}
# creating new id for sa
for (i in 1:nrow(sa)){
  sa$id[i] <- paste(sa[i,1], sa[i,3], sep = "_")
}
# using id to find where ca entries are appearing in sa
d.row <- which(sa$id %in% ca$id)
# naming the columns of sa and ca similarly
colnames(ca) <- c("prowess_code","company_name","year","sales","capital","cashflow","investment","debt","id")
colnames(sa) <- colnames(ca)
# merging the data
bs <- rbind(sa[-d.row,-9],ca[,-9])
# generating lagged value of capital and sales 
library(dplyr)
bs <- bs %>%
  group_by(prowess_code) %>%
  mutate(capital_1 = dplyr::lag(x = capital, n = 1, order_by = year), sales_1 = dplyr::lag(x = sales, n = 1, order_by = year), investment_1 = dplyr::lag(x = investment, n = 1, order_by = year), cashflow_1 = dplyr::lag(x = cashflow, n = 1, order_by = year), debt_1 = dplyr::lag(x = debt, n = 1, order_by = year))
# rearranging the data to get the group observations together arranged by year
bs <- bs %>%
  arrange(prowess_code, year)
# generating dependent and independent variables
# investment_it/capital_it-1 <- i.by.k_1
# cashflow_it/capital_it-1 <- cf.by.k_1
# sales_it-1/capital_it-1 <- s.by.k_1
# debt_it/capital_it-1 <- d.by.k_1
# investment_it-1/capital_it-2 <- i_1.by.k_2
bs <- bs %>%
  group_by(prowess_code) %>%
  mutate(i.by.k = investment/capital, cf_1.by.k_1 = cashflow_1/capital_1, s_1.by.k_1 = sales_1/capital_1, d_1.by.k_1 = debt_1/capital_1, i_1.by.k_1 = investment_1/capital_1, ds.by.k = (sales-sales_1)/capital)
# loading uncertainty index data
eui <- read.csv("./EUI_India.csv")
# creating annual uncertainty database
eui <- eui %>% 
  group_by(Year) %>%
  summarise(uncertainty = mean(India.News.Based.Policy.Uncertainty.Index))
colnames(eui)[1] <- "year"
# merging uncertainty data to bs
bs <- merge(x = bs, y = eui, by = "year", all.x = TRUE)
# rearranging the data for ease of analysis
bs <- bs %>%
  arrange(prowess_code, year)
# generating lagged variable of uncertainty
bs <- bs %>%
  group_by(prowess_code) %>%
  mutate(uncertainty_1 = dplyr::lag(x = uncertainty, order_by = year))
# generating PLI dummy
PLI <- NULL
for(i in 1:nrow(bs)){
  if (bs$year[i] == "2022" || bs$year[i] == "2023"){
    PLI[i] <- 1
  }
  else
    PLI[i] <- 0
}
bs <- cbind(bs,PLI)
colnames(bs)[22] <- "pli"
# adding repo rate data
repo_rate <- read.csv("./repo_rate.csv")
repo_rate[,1] <- format(base::as.Date(as.character(repo_rate[,1]), "%d-%m-%Y"), "%Y")
# "-" represents no change in value 
# so adding the same value as previous value in place of "-"
for (i in 1:nrow(repo_rate)){
  if (repo_rate[i,2] == "-"){
    repo_rate[i,2] <- repo_rate[i-1,2]
  }
}
repo_rate[,2] <- as.numeric(repo_rate[,2])
repo_rate <- repo_rate %>%
  group_by(date) %>%
  summarise(repo_rate = mean(repo_rate, na.rm = TRUE))
# addding repo_rate as a proxy for cost of capital to bs
colnames(repo_rate)[1] <- "year"
bs <- merge(x = bs, y= repo_rate, by = "year", all.x = TRUE)
# rearranging the data for ease of analysis
bs <- bs %>%
  arrange(prowess_code, year)
# creating a variable 
# change in repo_rate
bs <- bs %>%
  group_by(prowess_code) %>%
  mutate(d.repo_rate = repo_rate - dplyr::lag(x = repo_rate, order_by = year))
# running the panel data regression
library(AER)
library(plm)
library(stargazer)
# declaring the data as panel data
bs <- pdata.frame(bs, index = c("prowess_code","year"))
# running models
m1 <- pgmm(i.by.k ~ i_1.by.k_1 + cf_1.by.k_1 + s_1.by.k_1 + d_1.by.k_1 + uncertainty_1 + repo_rate | lag(i_1.by.k_1, 2:3), lag(cf_1.by.k_1, 1:2) + lag(s_1.by.k_1, 1:2) + lag(d_1.by.k_1, 1:2),
                   data = bs,
                   effect = "individual",
                   model = "twosteps",
                   transformation = "ld",
                   collapse = TRUE)
# AR(1) should be rejected as we expect serial autocorrelation 
# AR(2) should not be rejected as we expect no serial autocorrelation here
# basically the null AR(n) is that there is no serial autocorrelation
# sargan test has the null hypothesis that the instruments are valid instruments
# wald test has the null hypothesis that the instrumented variables are exogenous
# we would expect to reject the wald test hypothesis as we believe that the instrumented variables are endogenous
summary(m1, robust = TRUE)
m2 <- pgmm(i.by.k ~ i_1.by.k_1 + cf_1.by.k_1 + s_1.by.k_1 + d_1.by.k_1 + uncertainty_1 + repo_rate + pli| lag(i_1.by.k_1, 2:3), lag(cf_1.by.k_1, 1:2) + lag(s_1.by.k_1, 1:2) + lag(d_1.by.k_1, 1:2),
                     data = bs,
                     effect = "individual",
                     model = "twosteps",
                     transformation = "ld",
                     collapse = TRUE)
summary(m2, robust = TRUE)

m3 <- pgmm(i.by.k ~ i_1.by.k_1 + cf_1.by.k_1 + ds.by.k + d_1.by.k_1 + uncertainty_1 + repo_rate + pli | lag(i_1.by.k_1, 2:3), lag(cf_1.by.k_1, 1:2) + lag(ds.by.k, 1:2) + lag(d_1.by.k_1, 1:2),
                     data = bs,
                     effect = "individual",
                     model = "twosteps",
                     transformation = "ld",
                     collapse = TRUE)
summary(m3, robust = TRUE)

m4 <- pgmm(i.by.k ~ i_1.by.k_1 + cf_1.by.k_1 + s_1.by.k_1 + d_1.by.k_1 + uncertainty_1 + d.repo_rate + pli | lag(i_1.by.k_1, 2:3), lag(cf_1.by.k_1, 1:2) + lag(s_1.by.k_1, 1:2) + lag(d_1.by.k_1, 1:2),
                     data = bs,
                     effect = "individual",
                     model = "twosteps",
                     transformation = "ld",
                     collapse = TRUE)
summary(m4, robust = TRUE)

m5 <- pgmm(i.by.k ~ i_1.by.k_1 + cf_1.by.k_1 + ds.by.k + d_1.by.k_1 + uncertainty_1 + d.repo_rate + pli | lag(i_1.by.k_1, 2:3), lag(cf_1.by.k_1, 1:2) + lag(ds.by.k, 1:2) + lag(d_1.by.k_1, 1:2),
                     data = bs,
                     effect = "individual",
                     model = "twosteps",
                     transformation = "ld",
                     collapse = TRUE)
summary(m5, robust = TRUE)

rob_se1 <- list(sqrt(diag(vcovHC(m1, type = "HC1", method = "arellano", cluster = "group"))),
               sqrt(diag(vcovHC(m2, type = "HC1", method = "arellano", cluster = "group"))),
               sqrt(diag(vcovHC(m3, type = "HC1", method = "arellano", cluster = "group"))),
               sqrt(diag(vcovHC(m4, type = "HC1", method = "arellano", cluster = "group"))),
               sqrt(diag(vcovHC(m5, type = "HC1", method = "arellano", cluster = "group"))))
```

## Introduction

The objective of this analysis was to see whether there has been any effect of PLI on physical investment undertaken by beneficiary firms. To understand the same a regression analysis was performed. 

The data consists of consolidated annual financial statements of firms wherever available. If firms do not prepare a consolidated statement, the standalone statement was considered. 

Only beneficiaries of Category 1 are considered. This is because they were incentivised by the scheme to undertake greater investment. The selection criteria was based on investment plans, with those pledging greater investment being ranked higher.

The time period considered is FY15-FY23. Of these FY 22 and FY 23 are the years when the PLISFPI was implemented.

## Regression

In order to correct for endogeneity, GMM method is used in the literature^[See for example @panagiotidis_investment_2021, @rashid_firms_2017, @poncet_financial_2010, @gezici_determinants_2019, @saeed_bank_2012].

Therefore, GMM method was used for estimation. 

### Problem of multicolinearity

For a panel data model it is not possible to calculate a normal correlation matrix between the dependent variables. For the purposes of this analysis some indication of the correlation can be obtained by looking at a cross section.
This is dome below by drawing the correlation matrix for the FY 2019.

```{r correlation_matrix,echo=FALSE}
cor(subset(bs, year == "2019", select = c(i.by.k,cf_1.by.k_1,s_1.by.k_1,d_1.by.k_1,i_1.by.k_1,ds.by.k)))
```

The results show that there is a high correlation between cashflows and sales.
One way to look at the correlation matrix for the entire panel is to take the average of the correlation for all the cross sections. The direction of correlation varies from year to year, so that it is better to take an average of the absolute values.This is what is done next.

```{r abs_correlation_matrix, echo=FALSE}
# trying to calculate the correlation matrix for cross section data obtained from year FY 19
FY2019 <- cor(subset(bs, year == "2019", select = c(i.by.k,cf_1.by.k_1,s_1.by.k_1,d_1.by.k_1,i_1.by.k_1,ds.by.k)),use = "na.or.complete")
# calculating the correlation matrix for all other years and averaging it to get an idea about the panel.
FY2020 <- cor(subset(bs, year == "2020", select = c(i.by.k,cf_1.by.k_1,s_1.by.k_1,d_1.by.k_1,i_1.by.k_1,ds.by.k)),use = "na.or.complete")
FY2021 <- cor(subset(bs, year == "2021", select = c(i.by.k,cf_1.by.k_1,s_1.by.k_1,d_1.by.k_1,i_1.by.k_1,ds.by.k)),use = "na.or.complete")
FY2022 <- cor(subset(bs, year == "2022", select = c(i.by.k,cf_1.by.k_1,s_1.by.k_1,d_1.by.k_1,i_1.by.k_1,ds.by.k)),use = "na.or.complete")
FY2023 <- cor(subset(bs, year == "2023", select = c(i.by.k,cf_1.by.k_1,s_1.by.k_1,d_1.by.k_1,i_1.by.k_1,ds.by.k)),use = "na.or.complete")
FY2016 <- cor(subset(bs, year == "2016", select = c(i.by.k,cf_1.by.k_1,s_1.by.k_1,d_1.by.k_1,i_1.by.k_1,ds.by.k)), use = "na.or.complete")
FY2017 <- cor(subset(bs, year == "2017", select = c(i.by.k,cf_1.by.k_1,s_1.by.k_1,d_1.by.k_1,i_1.by.k_1,ds.by.k)), use = "na.or.complete")
FY2018 <- cor(subset(bs, year == "2018", select = c(i.by.k,cf_1.by.k_1,s_1.by.k_1,d_1.by.k_1,i_1.by.k_1,ds.by.k)), use = "na.or.complete")
mcor <- (abs(FY2016)+abs(FY2017)+abs(FY2018)+abs(FY2019)+abs(FY2020)+abs(FY2021)+abs(FY2022)+abs(FY2023))/8
mcor
```

The results again show that there is a high correlation between cashflows and sales. 
In the presence of imperfect multicollinaearity the t-ratios of the coefficients are not reliable for the correlated variables. However, as the variable of interest is not correlated with the other variables, there is no need to worry about the problem of collinearity. 

### Models

Following models were estimated: 

Model 1
$$
\begin{equation}
\tag{1}
\Big(\frac{I}{K}\Big)_{i,t} = \Big(\frac{I}{K}\Big)_{i,t-1} + \Big(\frac{CF}{K}\Big)_{i,t-1} + \Big(\frac{S}{K}\Big)_{i,t-1} + \Big(\frac{D}{K}\Big)_{i,t-1} + Uncertainty_{t-1} + RepoRate_{t}
\end{equation}
$$
Model 2
$$
\begin{equation}
\tag{2}
\Big(\frac{I}{K}\Big)_{i,t} = \Big(\frac{I}{K}\Big)_{i,t-1} + \Big(\frac{CF}{K}\Big)_{i,t-1} + \Big(\frac{S}{K}\Big)_{i,t-1} + \Big(\frac{D}{K}\Big)_{i,t-1} + Uncertainty_{t-1} + RepoRate_{t} + PLI_{t}
\end{equation}
$$
Model 3
$$
\begin{equation}
\tag{3}
\Big(\frac{I}{K}\Big)_{i,t} = \Big(\frac{I}{K}\Big)_{i,t-1} + \Big(\frac{CF}{K}\Big)_{i,t-1} + \Big(\frac{\Delta S}{K}\Big)_{i,t} + \Big(\frac{D}{K}\Big)_{i,t-1} + Uncertainty_{t-1} + RepoRate_{t} + PLI_{t}
\end{equation}
$$
Model 4
$$
\begin{equation}
\tag{4}
\Big(\frac{I}{K}\Big)_{i,t} = \Big(\frac{I}{K}\Big)_{i,t-1} + \Big(\frac{CF}{K}\Big)_{i,t-1} + \Big(\frac{S}{K}\Big)_{i,t-1} + \Big(\frac{D}{K}\Big)_{i,t-1} + Uncertainty_{t-1} + \Delta RepoRate_{t} + PLI_{t}
\end{equation}
$$
Model 5
$$
\begin{equation}
\tag{5}
\Big(\frac{I}{K}\Big)_{i,t} = \Big(\frac{I}{K}\Big)_{i,t-1} + \Big(\frac{CF}{K}\Big)_{i,t-1} + \Big(\frac{\Delta S}{K}\Big)_{i,t} + \Big(\frac{D}{K}\Big)_{i,t-1} + Uncertainty_{t-1} + \Delta RepoRate_{t} + PLI_{t}
\end{equation}
$$

The annual measure of uncertainty was taken by taken the average of the monthly Economic Policy Uncertainty Index by Scott Baker, Nicholos Bloom and Steven J. Davis.
The annual repo rate series was generated by averaging for quarterly figures available at the Database on Indian Economy maintained by the RBI.
Here, $I$ represents investment, $K$ represents capital stock, $CF$ represents cash flows, $S$ represents sales, $D$ represents debt and $\Delta$ represents change in a variable.
Year fixed effects other than PLI were sought to be captured through repo rate and uncertainty.

$I$ = Addition to Gross fixed Assets - Total addition to gross intangible assets

$D$ = Long term borrowings + Current portoin of long term borrowings

$S$ = Sale of goods

$K$ = Net fixed Assets

$CF$ = Net Cash flows from operating activities

```{r dpgmm, echo=FALSE}
library(stargazer)

stargazer(m1, 
          m2,
          m3,
          m4,
          m5,
          digits = 3,
          header = FALSE,
          type = "text", 
          se = rob_se1,
          title = "GMM Panel Regression Models of effect of PLI scheme on Category 1 beneficiary investment",
          model.numbers = FALSE,
          column.labels = c("(1)", "(2)","(3)","(4)","(5)"))
```

The results indicate that PLI has had no significant effect on the investment levels of the beneficiaries for all the specifications except one, where there is a significant positive effect at the 5% level.

### Measuring goodness of fit

In GMM estimation, we are not minimising the sum of error terms, so it is not a good idea to claculate the R square value. However, as a rule of thumb, some people calculate the correlation between actual and predicted values and square it to use as a pseudo R squared.


## References
